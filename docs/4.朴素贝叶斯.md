   
# 第4章 基于概率论的分类方法：朴素贝叶斯
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

![朴素贝叶斯_首页](/images/4.NaiveBayesian/朴素贝叶斯_首页.png "朴素贝叶斯首页")

## 使用概率分布进行分类

> 朴素贝叶斯简介

```
    前两章我们要求分类器做出艰难决策，给出“该数据实例属于哪一类”这类问题的明确答案。不过，分类器有时候会产生错误的结果，
    这时可以要求分类器给出一个最优的类别猜测结果，同时给出这个猜测的概率估计值。
    在这里我们先统计特征在数据集中取某个特定值的次数，然后除以数据集的实例总数，就得到了特征取该值的概率。
    下面我们会给出一些使用概率论进行分类的方法。首先从一个最简单的概率分类器开始，然后给出一些假设来学习朴素贝叶斯分类器。
    我们称之为“朴素”，是因为整个形式化过程只做最原始、最简答的假设。
```

> 贝叶斯决策理论

```
    朴素贝叶斯是贝叶斯决策理论的一部分，所以讲述朴素贝叶斯之前有必要快速了解一下贝叶斯决策理论。
    假设我们有一个数据集，它由两类数据组成，数据分布如下图所示：
```

![朴素贝叶斯示例数据分布](/images/4.NaiveBayesian/朴素贝叶斯示例数据分布.png "参数已知的概率分布")

```
    我们现在用p1(x,y)表示数据点(x,y)属于类别1（图中用圆点表示的类别）的概率，用p2(x,y)表示数据点(x,y)属于类别2（图中三角形表示的类别）的概率，
    那么对于一个新数据点(x,y)，可以用下面的规则来判断它的类别：
    * 如果 p1(x,y) > p2(x,y) ，那么类别为1
    * 如果 p2(x,y) > p1(x,y) ，那么类别为2
    也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。
```

> 朴素贝叶斯特点

```
    优点：在数据较少的情况下仍然有效，可以处理多类别问题。
    缺点：对于输入数据的准备方式较为敏感。
    适用于数据类型：标称型数据。
```

> 朴素贝叶斯的一般过程

```
    (1)收集数据：可以使用任何方法。本章使用RSS源。
    (2)准备数据：需要数值型或者布尔型数据。
    (3)分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好。
    (4)训练算法：计算不同的独立特征的条件概率。
    (5)测试算法：计算错误率。
    (6)使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。
```

## 学习朴素贝叶斯分类器

> 条件概率

![条件概率石头示例](/images/4.NaiveBayesian/贝叶斯条件概率.png "条件概率石头示例")

> 计算上述概率

```
    要计算 P(gray) 或者 P(black) ，事先得知道石头所在桶的信息会不会改变结果？你有可能已经想到计算从 B 桶中取到灰色石头的概率的办法，
    这就是所谓的条件概率 (conditional probability)。假定计算的是从 B 桶取到灰色石头的概率，这个概率可以记作 P(gray|bucketB)，
    我们称之为“在已知石头出自B桶的条件下，取出灰色石头的概率”。不难得到，P(gray|bucketA)值为2/4，P(gray|bucketB)的值为1/3。
    条件概率的计算公式如下所示：
    P(gray|bucketB) = P(gray and bucketB) / P(bucketB)
```
> 贝叶斯准则

```
    贝叶斯准则告诉我们如何交换条件概率中的条件和结果，即如果已知 P(x|c) ，要求 P(c|x) ，那么可以使用下面的计算方法：
    p(c|x) = p(x|c)·p(c)/p(x)
```

> 使用条件概率来分类

上面提到的贝叶斯决策理论要求计算两个概率 p1(x,y) 和 p2(x,y)：
* 如果 p1(x,y) > p2(x,y) ，那么属于类型1;
* 如果 p2(x,y) > p1(x,y) ，那么属于类型2;
但这两个准则并不是贝叶斯决策理论的所有内容。使用 p1() 和 p2() 只是为了尽可能简化描述，而真正需要计算和比较的是 p(c1|x,y) 和 p(c2|x,y)。
这些符号代表的具体意义是：给定某个由x,y表示的数据点，那么该数据点来自类别c1的概率是多少？数据点来自类别c2的概率又是多少？
注意这些概率与刚才给出的概率p(x,y|c1)并不一样，不过可以使用贝叶斯准则来交换概率中条件和结果。具体地，应用贝叶斯准则得到：
p(ci|x,y) = p(x,y|ci)·p(ci)/p(x,y)

使用这些定义，可以定义贝叶斯分类准则为：
* 如果 P(c1|x,y) > P(c2|x,y) ，那么属于类别c1;
* 如果 P(c1|x,y) < P(c2|x,y) ，那么属于类别c2;
使用贝叶斯准则，可以通过已知的三个概率值来计算未知的概率值。后面就会给出利用贝叶斯准则来计算概率并对数据进行分类的代码。现在介绍了一些概率理论，
你也了解了基于这些理论构建分类器的方法，接下来就要将它们付诸实践。 

## 解析RSS源数据

机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。
虽然电子邮件是一种会不断增加的文本，但我们同样也可以对新闻报道、用户留言、政府公文等其他任何类型的文本进行分类。
我们可以观察文档中出现的词，并把每个词的出现或者不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。
朴素贝叶斯是上节介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。

## 使用朴素贝叶斯来分析不同地区的态度

> 示例：使用朴素贝叶斯来发现低于相关的用词

```
    (1) 收集数据：从RSS源收集内容，这里需要对RSS源构建一个接口。
    (2) 准备数据：将文本文件解析成词条向量。
    (3) 分析数据：检查词条确保解析的正确性。
    (4) 训练算法：使用我们之前建立的 trainNB0()函数。
    (5) 测试算法：观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果。
    (6) 使用算法：构建一个完整的程序，封装所有内容。给定两个RSS源，该程序会显示最常用的公共词。
```

* 假设: 特征之间强（朴素）独立
* 概率模型
    * P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C) / P(F1F2...Fn)
   * 由于对于所有类别，P(F1F2...Fn)都是相同的，比较P(C|F1F2...Fn)只用比较P(F1F2...Fn|C)P(C)就好了
* 朴素贝叶斯的特点
    * 优点：在数据较少的情况下仍然有效，可以处理多类别问题
    * 缺点：对于输入数据的准备方式较为敏感
    * 适用数据类型：标称型数据
* 朴素贝叶斯的一般过程
    * 收集数据：可以使用任何方法
    * 准备数据：需要数值型或者布尔型数据
    * 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好。
    * 训练算法：计算不同的独立特征的条件概率
    * 测试算法：计算错误率
    * 使用算法：文本分类等
*  优化
    * 为了避免一个概率为0导致P(F1|C)*P(F2|C)....P(Fn|C)整个为0，所以优化为将所有词的出现数都初始化为1，并将分母初始化为2.
    * 由于大部分因子比较小，乘积之后得到的数不易比较，程序误差较大。所以取对数后可将乘法转化为加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))
* 总结
    * 这一块代码比较乱，最好先把公式理一理再看
    * 可以参考一下[阮一峰的博客](http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html)
