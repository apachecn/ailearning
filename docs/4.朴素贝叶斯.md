   
# 第4章 基于概率论的分类方法：朴素贝叶斯
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

![朴素贝叶斯_首页](/images/4.NaiveBayesian/NavieBayesian_headPage_xy.png "朴素贝叶斯首页")

## 使用概率分布进行分类

> 朴素贝叶斯简介

```
    前两章我们要求分类器做出艰难决策，给出“该数据实例属于哪一类”这类问题的明确答案。不过，分类器有时候会产生错误的结果，
    这时可以要求分类器给出一个最优的类别猜测结果，同时给出这个猜测的概率估计值。
    在这里我们先统计特征在数据集中取某个特定值的次数，然后除以数据集的实例总数，就得到了特征取该值的概率。
    下面我们会给出一些使用概率论进行分类的方法。首先从一个最简单的概率分类器开始，然后给出一些假设来学习朴素贝叶斯分类器。
    我们称之为“朴素”，是因为整个形式化过程只做最原始、最简答的假设。
```

> 贝叶斯决策理论

```
    朴素贝叶斯是贝叶斯决策理论的一部分，所以讲述朴素贝叶斯之前有必要快速了解一下贝叶斯决策理论。
    假设我们有一个数据集，它由两类数据组成，数据分布如下图所示：
```

![朴素贝叶斯示例数据分布](/images/4.NaiveBayesian/朴素贝叶斯示例数据分布.png "参数已知的概率分布")

```
    我们现在用p1(x,y)表示数据点(x,y)属于类别1（图中用圆点表示的类别）的概率，用p2(x,y)表示数据点(x,y)属于类别2（图中三角形表示的类别）的概率，
    那么对于一个新数据点(x,y)，可以用下面的规则来判断它的类别：
    * 如果 p1(x,y) > p2(x,y) ，那么类别为1
    * 如果 p2(x,y) > p1(x,y) ，那么类别为2
    也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。
```

> 朴素贝叶斯特点

```
    优点：在数据较少的情况下仍然有效，可以处理多类别问题。
    缺点：对于输入数据的准备方式较为敏感。
    适用于数据类型：标称型数据。
```

> 朴素贝叶斯的一般过程

```
    (1)收集数据：可以使用任何方法。本章使用RSS源。
    (2)准备数据：需要数值型或者布尔型数据。
    (3)分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好。
    (4)训练算法：计算不同的独立特征的条件概率。
    (5)测试算法：计算错误率。
    (6)使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。
```

## 学习朴素贝叶斯分类器

> 条件概率

![条件概率石头示例](/images/4.NaiveBayesian/贝叶斯条件概率.png "条件概率石头示例")

> 计算上述概率

```
    要计算 P(gray) 或者 P(black) ，事先得知道石头所在桶的信息会不会改变结果？你有可能已经想到计算从 B 桶中取到灰色石头的概率的办法，
    这就是所谓的条件概率 (conditional probability)。假定计算的是从 B 桶取到灰色石头的概率，这个概率可以记作 P(gray|bucketB)，
    我们称之为“在已知石头出自B桶的条件下，取出灰色石头的概率”。不难得到，P(gray|bucketA)值为2/4，P(gray|bucketB)的值为1/3。
    条件概率的计算公式如下所示：
    P(gray|bucketB) = P(gray and bucketB) / P(bucketB)
```
> 贝叶斯准则

```
    贝叶斯准则告诉我们如何交换条件概率中的条件和结果，即如果已知 P(x|c) ，要求 P(c|x) ，那么可以使用下面的计算方法：
    p(c|x) = p(x|c)·p(c)/p(x)
```

> 使用条件概率来分类

上面提到的贝叶斯决策理论要求计算两个概率 p1(x,y) 和 p2(x,y)：
* 如果 p1(x,y) > p2(x,y) ，那么属于类型1;
* 如果 p2(x,y) > p1(x,y) ，那么属于类型2;
但这两个准则并不是贝叶斯决策理论的所有内容。使用 p1() 和 p2() 只是为了尽可能简化描述，而真正需要计算和比较的是 p(c1|x,y) 和 p(c2|x,y)。
这些符号代表的具体意义是：给定某个由x,y表示的数据点，那么该数据点来自类别c1的概率是多少？数据点来自类别c2的概率又是多少？
注意这些概率与刚才给出的概率p(x,y|c1)并不一样，不过可以使用贝叶斯准则来交换概率中条件和结果。具体地，应用贝叶斯准则得到：
p(ci|x,y) = p(x,y|ci)·p(ci)/p(x,y)

使用这些定义，可以定义贝叶斯分类准则为：
* 如果 P(c1|x,y) > P(c2|x,y) ，那么属于类别c1;
* 如果 P(c1|x,y) < P(c2|x,y) ，那么属于类别c2;
使用贝叶斯准则，可以通过已知的三个概率值来计算未知的概率值。后面就会给出利用贝叶斯准则来计算概率并对数据进行分类的代码。现在介绍了一些概率理论，
你也了解了基于这些理论构建分类器的方法，接下来就要将它们付诸实践。 

## 解析RSS源数据

机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。
虽然电子邮件是一种会不断增加的文本，但我们同样也可以对新闻报道、用户留言、政府公文等其他任何类型的文本进行分类。
我们可以观察文档中出现的词，并把每个词的出现或者不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。
朴素贝叶斯是上节介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。

## 使用朴素贝叶斯来分析不同地区的态度

> 示例：使用朴素贝叶斯来发现低于相关的用词

```
    (1) 收集数据：从RSS源收集内容，这里需要对RSS源构建一个接口。
    (2) 准备数据：将文本文件解析成词条向量。
    (3) 分析数据：检查词条确保解析的正确性。
    (4) 训练算法：使用我们之前建立的 trainNB0()函数。
    (5) 测试算法：观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果。
    (6) 使用算法：构建一个完整的程序，封装所有内容。给定两个RSS源，该程序会显示最常用的公共词。
```

* 假设: 特征之间强（朴素）独立
* 概率模型
    * P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C) / P(F1F2...Fn)
   * 由于对于所有类别，P(F1F2...Fn)都是相同的，比较P(C|F1F2...Fn)只用比较P(F1F2...Fn|C)P(C)就好了
* 朴素贝叶斯的特点
    * 优点：在数据较少的情况下仍然有效，可以处理多类别问题
    * 缺点：对于输入数据的准备方式较为敏感
    * 适用数据类型：标称型数据
* 朴素贝叶斯的一般过程
    * 收集数据：可以使用任何方法
    * 准备数据：需要数值型或者布尔型数据
    * 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好。
    * 训练算法：计算不同的独立特征的条件概率
    * 测试算法：计算错误率
    * 使用算法：文本分类等
*  优化
    * 为了避免一个概率为0导致P(F1|C)*P(F2|C)....P(Fn|C)整个为0，所以优化为将所有词的出现数都初始化为1，并将分母初始化为2.
    * 由于大部分因子比较小，乘积之后得到的数不易比较，程序误差较大。所以取对数后可将乘法转化为加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))
* 总结
    * 这一块代码比较乱，最好先把公式理一理再看
    * 可以参考一下[阮一峰的博客](http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html)
    * 对于分类而言，使用概率有时要比使用硬规则更为有效。贝叶斯概率及贝叶斯准则提供了一种利用已知值来估计未知概率的有效方法。
    * 可以通过特征之间的条件独立性假设，降低对数据量的需求。独立性假设是指一个词的出现概率并不依赖于文档中的其他词。当然我们也知道这个假设过于简单。
    这就是之所以成为朴素贝叶斯的原因。尽管条件独立性假设并不正确，但是朴素贝叶斯仍然是一种有效的分类器。
    * 利用现代编程语言来实现朴素贝叶斯时需要考虑很多实际因素。下溢出就是其中一个问题，它可以通过对概率取对数来解决。

* * *

* **作者：[羊三](http://www.apache.wiki/display/~xuxin) [小瑶](http://www.apache.wiki/display/~chenyao)**
* [GitHub地址](https://github.com/apachecn/MachineLearning): <https://github.com/apachecn/MachineLearning>
* **版权声明：欢迎转载学习 => 请标注信息来源于 [ApacheCN](http://www.apache.wiki)**

