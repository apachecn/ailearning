
# 第3章 决策树
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

![决策树_首页](/images/3.DecisionTree/DecisionTree_headpage_xy.png "决策树首页")

## 决策树简介

![决策树-流程图](/images/3.DecisionTree/决策树-流程图.jpg "决策树示例流程图")

> 决策树的任务

```
    第二章的k-近邻算法可以完成很多分类任务，但是它最大的缺点就是无法给出数据的内在含义。
    决策树的主要优势就在于数据形式非常容易理解。
    接下来构造的决策树算法能够读取数据集合，构建类似于上图的决策树。决策树的一个重要任务是为了理解数据中所蕴含的知识信息，
    因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，这些机器根据数据集创建规则的过程，就是机器学习的过程。
    专家系统中经常使用决策树，而且决策树给出结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。
```

> 决策树的特点

```
    优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
    缺点：可能会产生过度匹配问题。
    适用数据类型：数值型和标称型。
```

## 在数据集中度量一致性

## 使用递归构造决策树

> 构造决策树时需要解决的第一个问题

```
    在构造决策树时，我们需要解决的第一个问题就是，当前的数据集上哪个特征在划分数据分类时起决定性作用。
为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。
这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则当前无需阅读的垃圾邮件已经正确地划分数据分类，
无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程。划分数据子集的算法和划分原始数据及的方法相同，
直到所有具有相同类型的数据均在一个数据子集内。
```

> 创建分支的伪代码函数createBranch()

```
    检测数据集中的每个子项是否属于同一分类：
        If so return 类标签
        Else
            寻找划分数据集的最好特征
            划分数据集
            创建分支节点
                for 每个划分的子集
                    调用函数createBranch并增加返回结果到分支节点中
            return 分支节点
```

> 决策树的一般流程

```
    (1)收集数据：可以使用任何方法。
    (2)准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。
    (3)分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
    (4)训练算法：构造树的数据结构。
    (5)测试算法：使用经验树计算错误率。
    (6)使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。
```

> 划分数据集时的数据路径

```
    得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。
    第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上，我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。
    递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。如果所有实例具有相同的分类，
    则得到一个叶子节点或者终止块。任何到达叶子节点的数据必然属于叶子节点的分类，如下图所示：
```

![决策树划分数据集时的数据路径](/images/3.DecisionTree/决策树划分数据集时的数据路径.png)

## 使用Matplotlib绘制树形图

> Matplotlib绘制树形图示例

![Matplotlib绘制树形图示例](/images/3.DecisionTree/Matplotlib绘制树形图.png)

* 决策树是什么？
    * 顾名思义，是一种树，一种依托于策略抉择而建立起来的树。
    * 从数据产生决策树的机器学习技术叫做决策树学习, 通俗点说就是决策树。
* 决策数目前的情况：
    * 1.最经常使用的数据挖掘算法。(流行的原因：不需要了解机器学习的知识，就能搞明白决策树是如何工作的)
    * 2.数据形式[决策过程只有：是／否]和数据内在含义非常容易理解。
    * 3.决策树给出的结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。
* 决策树的构造：
    * 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
    * 缺点：可能会产生过度匹配问题。
    * 适用数据类型：数值型和标称型[标称型:其实就是离散型数据，变量的结果只在`有限`目标集中取值(例如：分类特征 A/B/C类其中一种)]。
* 如何找出第一个分支点呢？
    * 信息增益： 
        * 划分数据集的最大原则是：将无序的数据变得更加有序。
        * 集合信息的度量称为`香农熵`或者简称`熵`(名字来源于信息论之父`克劳德·香农`)
        * 公式： 
            * \\(p(x_i)\\) 表示该label分类的概率
            * \\(l(x_i) = - \log_2p(x_i)\\) 表示符号\\(x_i\\)的信息定义
            * \\(H = -\sum_{i=0}^np(x_i)\log_2p(x_i)\\) 表示香农熵，用于计算信息熵
    * 基尼不纯度(Gini impurity)  [本书不做过多的介绍]
        * 简单来说：就是从一个数据集中随机选取子项，度量其被错误分类到其他分组里的概率。
* 流程介绍图
* ![决策树流程介绍图](/images/3.DecisionTree/决策树流程介绍图.jpg)

* * *

* **作者：[片刻](http://www.apache.wiki/display/~jiangzhonglian) [小瑶](http://www.apache.wiki/display/~chenyao)**
* [GitHub地址](https://github.com/apachecn/MachineLearning): <https://github.com/apachecn/MachineLearning>
* **版权声明：欢迎转载学习 => 请标注信息来源于 [ApacheCN](http://www.apache.wiki)**        