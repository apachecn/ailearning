# 6) 支持向量机
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

![支持向量机_首页](/images/6.SVM/SVM_1.jpg)

## 支持向量机的概念

> 支持向量机(Support Vector Machines, SVM)

* 支持向量(Support Vector)就是离分隔超平面最近的那些点。
* 机(Machine)就是表示一种算法，而不是表示机器。
* SVM有很多种实现，最流行的一种实现是： `序列最小优化(Sequential Minimal Optimization, SMO)算法`。
* 下面还会介绍一种称为`核函数(kernel)`的方式将SVM扩展到更多数据集上。
* 实战项目：回顾第1章中手写识别的案例，并考察其能否通过SVM来提高识别的效果。
* 注意：`SVM几何含义比较直观，但其算法实现较复杂，牵扯大量数学公式的推导。`

```
优点：泛化错误率低，计算开销不大，结果易理解。
缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适合于处理二分类问题。
使用数据类型：数值型和标称型数据。
```

## 基于最大间隔分隔数据

* 数据可以通过画一条直线就可以将它们完全分开，这组数据叫`线性可分(linearly separable)`数据。
* 而这条分隔直线称为`分隔超平面(separating hyperplane)`。
* 如果数据集上升到1024维呢？那么需要1023维来分隔数据集，也就说需要N-1维的对象来分隔，这个对象叫做`超平面(hyperlane)`，也就是分类的决策边界。
* ![分隔超平面](/images/6.SVM/SVM_2_separating-hyperplane.jpg)

## 寻找最大间隔

> 为什么寻找最大间隔

```
摘录地址：http://slideplayer.com/slide/8610144  (第12条信息)
Support Vector Machines: Slide 12 Copyright © 2001, 2003, Andrew W. Moore Why Maximum Margin? 
denotes +1 denotes -1 f(x,w,b) = sign(w. x - b) The maximum margin linear classifier is the linear classifier with the, um, maximum margin. 
This is the simplest kind of SVM (Called an LSVM) Support Vectors are those datapoints that the margin pushes up against 

1.Intuitively this feels safest. 
2.If we’ve made a small error in the location of the boundary (it’s been jolted in its perpendicular direction) this gives us least chance of causing a misclassification. 
3.CV is easy since the model is immune to removal of any non-support-vector datapoints. 
4.There’s some theory that this is a good thing. 
5.Empirically it works very very well. 

* * *

1. 直觉上是安全的
2. 如果我们在边界的位置发生了一个小错误（它在垂直方向上被颠倒），这给我们最小的错误分类机会。
3. CV很容易，因为该模型对任何非支持向量数据点的去除是免疫的。
4. 有一些理论，这是一件好事。
5. 通常它的工作非常好。
```

* 选择D会比B、C分隔的效果要好很多，原因是上述的5个结论。
* 所有的点看作地雷吧，那么我们(超平面)得找到最近所有的地雷，并保证我们离它最远。
![线性可分](/images/6.SVM/SVM_3_linearly-separable.jpg)

### 怎么寻找最大间隔

> 点到超平面的距离

* 分隔超平面`函数间距`:  \\(y(x)=w^Tx+b\\)
* 分类的结果： \\(f(x)=sign(w^Tx+b)\\)  (sign表示>0为1，<0为-1，=0为0) 
* 点到超平面的`几何间距`: \\(d(x)=(w^Tx+b)/||w||\\)  （||w||表示w矩阵的二范式=> \\(\sqrt{w*w^T}\\), 点到超平面的距离也是类似的，需要复习一下向量的知识）
![点到直线的几何距离](/images/6.SVM/SVM_4_point2line-distance.png)

> 拉格朗日乘子法的使用

* 类别标签用-1、1，是为了后期方便 \\(lable*(w^Tx+b)\\) 的标识和距离计算；如果 \\(lable*(w^Tx+b)>0\\) 表示预测正确，否则预测错误。
* 现在目标很明确，就是要找到`w`和`b`，因此我们必须要找到最小间隔的数据点，也就是前面所说的`支持向量`。
    * 也就说，让最小的距离取最大.(最小的距离：就是最小间隔的数据点；最大：就是最大间距，为了找出最优超平面--最终就是支持向量)
    * 怎么理解呢？ 例如： 所有的点看作地雷吧，那么我们(超平面)得找到最近所有的地雷，并保证我们离它最远。
    * 目标函数：\\(arg: max\ \{min\ [lable*(w^Tx+b)/||w||]\}\\)
    * 1.如果 \\(lable*(w^Tx+b)>0\\) 表示预测正确，也称`函数间隔`，\\(||w||\\) 可以理解为归一化，也称`几何间隔`，我们始终可以找到一个阈值让 \\(lable*(w^Tx+b)>=1\\)
    * 2.所以令 \\(lable*(w^Tx+b)=1\\)，我们本质上是求 \\(arg: max\{关于w, b\}(1/||w||)\\)；也就说，我们约束(前提)条件是: \\(lable*(w^Tx+b)>=1\\)
* 新的目标函数求解： \\(arg: max\{关于w, b\}\ (1/||w||)\\)
    * => 就是求: \\(arg: min\{关于w, b\}\ (||w||)\\) (求矩阵会比较麻烦，如果x只是1/2*X^2的偏导数，那么。。同样是求最小值)
    * => 就是求: \\(arg: min\{关于w, b\}\ (\frac{1}{2}*||w||^2)\\) (二次函数求导，求极值，平方也方便计算)
    * 本质上就是求线性不等式的二次优化问题(求分隔超平面，等价于求解相应的凸二次规划问题。)
* 通过拉格朗日乘子法，求二次优化问题
    * 假设需要求极值的目标函数 (objective function) 为 f(x,y)，限制条件为 φ(x,y)=M  # M>=1
    * 设g(x,y)=M-φ(x,y)   # 临时φ(x,y)表示下文中 \\(label*(w^Tx+b)\\)
    * 定义一个新函数: F(x,y,λ)=f(x,y)+λg(x,y)
    * a为λ，代表要引入的拉格朗日乘子(Lagrange multiplier)
    * 那么： \\(L(w,b,\alpha)=\frac{1}{2} * ||w||^2 + \sum_{i=1}^{n} \alpha * [1 - label * (w^Tx+b)]\\)  => \\(L(w,b,\alpha)=\frac{1}{2} * ||w||^2 - \sum_{i=1}^{n} \alpha * [label * (w^Tx+b) - 1]\\) 
    * 因为：\\(label*(w^Tx+b)>=1, \alpha>0\\) , 所以 \\(\alpha*[label*(w^Tx+b)-1]>=0\\), \\(\sum_{i=1}^{n} \alpha * [label * (w^Tx+b) - 1]>=0\\) 
    * \\(max\{关于\alpha\}\ L(w,b,\alpha) = \frac{1}{2} *||w||^2\\) 
    * 如果求： \\(min\{关于w, b\}\ \frac{1}{2} *||w||^2\\) , 也就是要求： \\(min\{关于w, b\}\ [max\{关于\alpha\}\ L(w,b,\alpha)]\\) 
* 现在转化到对偶问题的求解
    * \\(min\{关于w, b\}\ [max\{关于\alpha\}\ L(w,b,\alpha)]\\) >= \\(max\{关于\alpha\}\ [min\{关于w, b\}\ L(w,b,\alpha)]\\) 
    * 现在分2步
    * 先求： \\(min\{关于w, b\}\ L(w,b,\alpha)=\frac{1}{2} * ||w||^2 + \sum_{i=1}^{n} \alpha * [1 - label * (w^Tx+b)]\\)
    * 就是求`L(w,b,a)`关于[w, b]的偏导数, 得到`w和b的值`，并化简为：`L和a的方程`。
    * 参考： 如果公式推导还是不懂，也可以参考《统计学习方法》李航-P103<学习的对偶算法>
    * ![计算拉格朗日函数的对偶函数](/images/6.SVM/SVM_5_Lagrangemultiplier.png)
* 终于得到课本上的公式： \\(max\ \{\alpha\}\ [\sum_{i=1}^{m} \alpha - \frac{1}{2} \sum_{i, j=1}^{m} label_i·label_j·\alpha_i·\alpha_j·<x_i, x_j>]\\)
* 约束条件： \\(a>=0,\ and\ \sum_{i=1}^{m} a_i·label_i=0\\)

> 松弛变量(slack variable)

* 我们知道几乎所有的数据都不那么干净, 通过引入松弛变量来允许数据点可以处于分隔面错误的一侧。
* 约束条件： \\(C>=a>=0,\ and\ \sum_{i=1}^{m} a_i·label_i=0\\)
* 这里常量C用于控制“最大化间隔”和“保证大部分点的函数间隔小于1.0” 这两个目标的权重。
* 常量C是一个常数，我们通过调节钙参数得到不同的结果。一旦求出了所有的alpha，那么分隔超平面就可以通过这些alpha来表示。
* 这一结论十分直接，SVM中的主要工作就是要求解 alpha.

> SVM应用的一般框架

```
SVM的一般流程
收集数据：可以使用任意方法。
准备数据：需要数值型数据。
分析数据：有助于可视化分隔超平面。
训练算法：SVM的大部分时间都源自训练，该过程主要实现两个参数的调优。
测试算法：十分简单的计算过程就可以实现。
使用算法：几乎所有分类问题都可以使用SVM，值得一提的是，SVM本身是一个二类分类器，对多类问题应用SVM需要对代码做一些修改。
```
* 到目前为止，我们已经了解了一些理论知识，现在我们通过`Code`来实现我们的算法吧。

## SMO高效优化算法

> 序列最小优化(Sequential Minimal Optimization, SMO)

* 创建作者：John Platt
* 创建时间：1996年
* SMO用途：用于训练SVM
* SMO目标：求出一系列alpha和b,一旦求出alpha，就很容易计算出权重向量w并得到分隔超平面。
* SMO思想：是讲大优化问题分解为多个小优化问题来求解的。
* SMO原理：每次循环选择两个alpha进行优化处理，一旦找出一对合适的alpha，那么就增大一个同时减少一个。
    * 这里指的合适必须要符合一定的条件
        * 1.这两个alpha必须要在间隔边界之外
        * 2.这两个alpha还没有进行过区间化处理或者不在边界上。
    * 之所以要同时改变2个alpha；原因，我们有一个约束条件：`Σ a[i]*label(i)=0`；如果只是修改一个alpha，很可能导致约束条件失效。

```
SMO伪代码大致如下：

创建一个alpha向量并将其初始化为0向量
当迭代次数小于最大迭代次数时(外循环)
    对数据集中的每个数据向量(内循环)：
        如果该数据向量可以被优化
            随机选择另外一个数据向量
            同时优化这两个向量
            如果两个向量都不能被优化，退出内循环
    如果所有向量都没被优化，增加迭代数目，继续下一次循环
```

> SVM简化版：应用简化版SMO算法处理小规模数据集

代码可参考 svm-simple.py

> SVM完整版：使用完整 Platt SMO算法加速优化

代码可参考 svm-complete_Non-Kernel.py
* 优化点：选择alpha的方式不同。

## 在复杂数据上应用核函数

* 对于线性可分的情况，效果明显
* 对于非线性的情况也一样，此时需要用到一种叫`核函数(kernel)`的工具将数据转化为分类器易于理解的形式。

> 利用核函数将数据映射到高维空间

* 使用核函数：可以将数据从某个特征空间到另一个特征空间的映射。（通常情况下：这种映射会将低维特征空间映射到高维空间。）
* 如果觉得特征空间很装逼、很难理解。
* 可以把核函数想象成一个包装器(wrapper)或者是接口(interface)，它能将数据从某个很难处理的形式转换成为另一个较容易处理的形式。
* 经过空间转换后：低维需要解决的非线性问题，就变成了高维需要解决的线性问题。
* SVM优化特别好的地方，在于所有的运算都可以写成内积(inner product: 是指2个向量相乘，得到单个标量 或者 数值)；内核替换成核函数的方式被称为`核技巧(kernel trick)`或者`核"变电"(kernel substation)`
* 核函数并不仅仅应用于支持向量机，很多其他的机器学习算法也都用到核函数。最流行的核函数：径向基函数(radial basis function)
* 径向基函数的高斯版本，其具体的公式为：
* ![径向基函数的高斯版本](/images/6.SVM/SVM_6_radial-basis-function.jpg)
