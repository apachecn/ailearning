
# 5) 逻辑回归基础

  * 逻辑回归(Logistic Regression)
    * 5.1 分类问题
        * 在分类问题中，尝试预测的是结果是否属于某一个类（例如正确或错误）。
        * 分类问题的例子有：
            * 判断一封电子邮件是否是垃圾邮件；
            * 判断一次金融交易是否是欺诈等等。
        * 从二元的分类问题开始讨论:
             将因变量(dependant variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量
             y属于{0，1}
             注：其中 0 表示负向类，1 表示正向类。
    * 5.2 假说表示

    * 5.3 判定边界
        * 在逻辑回归中，我们预测：
             当 hθ 大于等于 0.5 时，预测 y=1
             当 hθ 小于 0.5 时，预测 y=0
        * 根据上面绘制出的 S 形函数图像，我们知道当
             z=0时 ，g(z)=0.5
             z>0时 ，g(z)>0.5
             z<0时 ，g(z)<0.5
             又z=θ的T次方与X的积，即：
               z大于等于0时，预测：y=1
               z小于0时，预测：y=0
        * 现在假设我们有一个模型：Hθ(x)=g(θ0+θ1*x1+θ2*x2)
             并且参数θ是向量[-3 1 1]。则当-3+x1+x2大于等于0，即x1+x2大于等于3时，模型将预测y=1。
             我们可以绘制直线x1+x2=3，这条线便是我们模型的分界线，将预测为1的区域和预测为0的区域分隔开。
        * 假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？
          因为需要用曲线才能分隔 y=0 的区域和 y=1 的区域，我们需要二次方特征： 假设参数是Hθ(x)=g(θ0+θ1*x1+θ2*x2+θ3*(x1^2)+θ4*(x2^2)+θ4*(x2^2))
          是[-1 0 0 1 1]，则我们得到的判定边界恰好是圆点在原点且半径为 1 的圆形。可以用非常复杂的模型来适应非常复杂形状的判定边界。
    * 5.4 代价函数
    * 5.5 简化的成本函数和梯度下降
    * 5.6 高级优化
    * 5.7 多类分类：一个对所有
