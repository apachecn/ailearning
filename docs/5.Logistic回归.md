# 第5章 Logistic回归
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

![朴素贝叶斯_首页](/images/5.Logistic/LogisticRegression_headPage_xy.png "Logistic回归首页")

## Sigmoid函数和Logistic回归分类器

> 回归简介

```
    假设现在有一些数据点，我们用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合过程就称作回归。
    Logistic 回归主要的用途是用来做分类，利用 Logistic 回归 进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。
    我们这里所说的“回归”一词源于最佳拟合，表示要找到最佳拟合参数集。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法。
```

> Logistic回归特点

```
    优点：计算代价不高，易于理解和实现。
    缺点：容易欠拟合，分类精度可能不高。
    适用数据类型：数值型和标称型数据。
```

> Sigmoid函数简介

```
    我们想要的函数应该是，能接受所有的输入然后预测出类别。例如，在两个类的情况下，上述函数输出 0 和 1 。这类函数称为海维塞得阶跃函数，或者直接称之为 单位阶跃函数。
    但是，海维塞得阶跃函数的问题在于：该函数在跳跃点上从 0 瞬间跳跃到 1，这个瞬间跳跃过程有时候很难处理。幸好，另外的一个函数也有这样的性质(这里的性质指的是可以输出0和1的性质),
    且数学上更易处理，这就是我们下边要介绍的 Sigmoid 函数。

    Sigmoid函数具体的计算公式如下：
    f(z) = 1 / (1 + e ^(-z))
    图5-1 给出了Sigmoid函数在不同坐标尺度下的两条曲线图。当x为0时，Sigmoid函数值为0.5。随着x的增大，对应的Sigmoid值将逼近1；而随着x的减小，Sigmoid值将逼近0。
    如果横坐标刻度足够大（图5-1下图），Sigmoid函数看起来很像一个阶跃函数。
    因此，为了实现Logistic回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把所有的结果值相加，将这个总和带入Sigmoid函数中，进而得到一个范围在0~1之间的数值。
    任何大于0.5的数据被分入1类，小于0.5即被归入0类。所以，Logistic回归也可以被看成是一种概率估计。
```
![Logistic回归Sigmoid函数](/images/5.Logistic/Logistic回归Sigmoid函数.png "Logistic回归Sigmoid函数")

## 最优化理论初步

```
    Sigmoid函数的输入记为z，由下面的公式得出
        z = w0x0 + w1x1 + w2x2 + ... +wnxn    
    如果采用向量的写法，上述公式可以写成 z = wTx ,它表示将这两个数值向量对应元素相乘然后全部加起来即得到z值。其中的向量 x 是分类器的输入数据，
    向量 w 也就是我们要找到的最佳参数（系数），从而使得分类器尽可能地精确。为了寻找最佳参数，需要用到最优化理论的一些知识。
```                                                                                                                                                                                              

> 梯度上升法

```
    梯度上升法基于的思想是：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为▽，则函数f(x,y)的梯度由下式表示：
```

![logistic回归梯度上升法](/images/5.Logistic/梯度上升算法.png "梯度上升法")

介绍一下几个相关的概念：
```
    例如：y = w1x1 + w2x2 + ... + wnxn
    梯度：参考上图的例子，二维图像，x方向是代表第一个系数，也就是w1，y方向代表第二个系数也就是w2，这样的向量就是梯度。
    α：上面的梯度算法的迭代公式中的阿尔法，这个代表的是移动步长。移动步长会影响最终结果的拟合程度，最好的方法就是随着迭代次数更改移动步长。
        步长通俗的理解，100米，如果我一步走10米，我需要走10步；如果一步走20米，我只需要走5步。这里的一步走多少米就是步长的意思。
    ▽f(w)：代表沿着梯度变化的方向。
```

拟合程度简介
参考： http://blog.csdn.net/willduan1/article/details/53070777

下面是原始数据集：

![拟合程度图](/images/5.Logistic/拟合程度示例图.png "拟合程度示例图")

下面是拟合程度较好的：

![拟合程度较好示例图](/images/5.Logistic/拟合程度较好示例图.png "拟合程度较好示例图")

欠拟合：模型没有很好地捕捉到数据特征，不能很好地拟合数据。

![欠拟合示例图](/images/5.Logistic/欠拟合示例图.png "欠拟合示例图")

过拟合：模型把数据学习的太彻底了，以至于把噪声数据的特征也学习到了，这样就会导致在后期测试的时候不能够很好地识别数据，即不能正确地分类，模型泛化能力太差。

![过拟合示例图](/images/5.Logistic/过拟合示例图.png "过拟合示例图")

梯度上升法的伪代码如下：
```
    某个回归系数初始化为1
    重复R次：
        计算整个数据集的梯度
        使用 alpha X grandient 更新回归系数的向量
    返回回归系数
```


梯度上升算法在每次回归系数时都需要遍历整个数据集，该方法在处理100个左右的数据集时尚可，但如果有数十亿样本和成千上万的特征，
那么该方法的计算复杂度就太高了。一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为随机梯度上升算法。
由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习算法。与“在线学习”相对应，
一次处理所有数据被称作是“批处理”。
    随机梯度上升算法可以写成如下的伪代码：

    所有回归系数初始化为1
    对数据集中每个样本
        计算该样本的梯度
        使用 alpha X gradient 更新回归系数值
    返回回归系数值

## 梯度下降最优化算法

你最经常听到的应该是梯度下降算法，它与这里的梯度上升算法是一样的，只是公式中的加法需要变成减法。因此，对应的公式可以写成：
    w:=w-a▽f(w)
梯度上升算法用来求函数的最大值，而梯度下降算法用来求函数的最小值。

## 数据中的缺失项处理

数据中的缺失值是个非常棘手的问题，有很多文献都致力于解决这个问题。这个问题没有标准答案，取决于实际应用中的需求。
那么，数据缺失究竟带来了什么问题？假设有100个样本和20个特征，
这些数据都是机器手机回来的。若机器上的某个传感器损坏导致一个特征无效时该怎么办？此时是否要扔掉整个数据？这种情况下，另外19个特征怎么办？
它们是否还可以用？答案是肯定的。因为有时候数据相当昂贵，扔掉和重新获取都是不可取的，所以必须采用一些方法来解决这个问题。
* 下面给出了一些可选的做法：
    * 使用可用特征的均值来填补缺失值；
    * 使用特殊值来填补缺失值，如 -1；
    * 忽略有缺失值的样本；
    * 使用有相似样本的均值添补缺失值；
    * 使用另外的机器学习算法预测缺失值。

## Logistic 回归总结

* 逻辑回归(Logistic Regression)
    * 5.1 分类问题
        * 在分类问题中，尝试预测的是结果是否属于某一个类（例如正确或错误）。
        * 分类问题的例子有：
            * 判断一封电子邮件是否是垃圾邮件；
            * 判断一次金融交易是否是欺诈等等。
        * 从二元的分类问题开始讨论:
             将因变量(dependant variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量
             y属于{0，1}
             注：其中 0 表示负向类，1 表示正向类。
    * 5.2 假说表示

    * 5.3 判定边界
        * 在逻辑回归中，我们预测：
        
             当 hθ 大于等于 0.5 时，预测 y=1
             当 hθ 小于 0.5 时，预测 y=0
        * 根据上面绘制出的 S 形函数图像，我们知道当
             z=0时 ，g(z)=0.5
             z>0时 ，g(z)>0.5
             z<0时 ，g(z)<0.5
             又z=θ的T次方与X的积，即：
               z大于等于0时，预测：y=1
               z小于0时，预测：y=0
        * 现在假设我们有一个模型：Hθ(x)=g(θ0+θ1*x1+θ2*x2)
             并且参数θ是向量[-3 1 1]。则当-3+x1+x2大于等于0，即x1+x2大于等于3时，模型将预测y=1。
             我们可以绘制直线x1+x2=3，这条线便是我们模型的分界线，将预测为1的区域和预测为0的区域分隔开。
        * 假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？
          因为需要用曲线才能分隔 y=0 的区域和 y=1 的区域，我们需要二次方特征： 假设参数是Hθ(x)=g(θ0+θ1*x1+θ2*x2+θ3*(x1^2)+θ4*(x2^2)+θ4*(x2^2))
          是[-1 0 0 1 1]，则我们得到的判定边界恰好是圆点在原点且半径为 1 的圆形。可以用非常复杂的模型来适应非常复杂形状的判定边界。
    * 5.4 代价函数
    * 5.5 简化的成本函数和梯度下降
    * 5.6 高级优化

## Logistic 本章小结

```
    Logistic回归的目的是寻找一个非线性函数 Sigmoid 的最佳拟合参数，求解过程可以由最优化算法来完成。
    在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升算法。
    随机梯度上升算法与梯度上升算法的效果相当，但占用更少的计算资源。
    此外，随机梯度上升是一个在线算法，它可以在新数据到来时就完成参数更新，而不需要重新读取整个数据集来进行批处理运算。
```

* * *

* **作者：[羊三](http://www.apache.wiki/display/~xuxin) [小瑶](http://www.apache.wiki/display/~chenyao)**
* [GitHub地址](https://github.com/apachecn/MachineLearning): <https://github.com/apachecn/MachineLearning>
* **版权声明：欢迎转载学习 => 请标注信息来源于 [ApacheCN](http://www.apache.wiki)**
